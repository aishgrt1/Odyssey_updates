{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odyssey.core.bigquery.BigQueryGithubEntry import *\n",
    "from odyssey.core.bigquery.GithubPython import GithubPython\n",
    "from odyssey.core.analyzer import InstantiationAnalyzer\n",
    "from odyssey.core.analyzer import ImportAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nbconvert import PythonExporter\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from odyssey.core.analyzer import InstantiationAnalyzer\n",
    "import glob\n",
    "import ast\n",
    "import jedi\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from odyssey.core.bigquery.GithubPython import GithubPython as ghp\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionCallVisitor(ast.NodeVisitor):\n",
    "    def visit_Call(self, node):\n",
    "        print (ast.dump(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "functionsList=['RF', 'GridSearchCV','make_pipeline','AdaBoostRegressor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting files in repo containing scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\\\n",
    "SELECT\n",
    "    DISTINCT(repo_name)\n",
    "FROM\n",
    "    `Odyssey_github_sklearn.content_py_full` \n",
    "WHERE\n",
    "    (REGEXP_CONTAINS(path,\"sklearn\")) OR\n",
    "    (REGEXP_CONTAINS(repo_name,\"scikit-learn\")) OR\n",
    "    (REGEXP_CONTAINS(repo_name,\"sklearn\")) OR\n",
    "    (REGEXP_CONTAINS(path,\"scikit-learn\"))\n",
    "\"\"\"\n",
    "gp = GithubPython()\n",
    "\n",
    "\n",
    "\n",
    "excludedRepos = [repo_name[0] for repo_name in gp.run(query, project=\"odyssey-193217\")]\n",
    "print(excludedRepos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excludeByRepoFull(excludeByRepos):\n",
    "    if not excludedRepos:\n",
    "        return \"\"\n",
    "    s = '(NOT REGEXP_CONTAINS(repo_name,\"%s\"))' % excludedRepos[0]\n",
    "    for repo in excludedRepos[1:]:\n",
    "        s += ' AND '\n",
    "        s += '(NOT REGEXP_CONTAINS(repo_name,\"%s\"))' % repo\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "functionListFromFile= pd.read_csv(\"classNames.csv\")\n",
    "functionListFromFile.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARDRegression\n",
      "AdaBoostClassifier\n",
      "AdaBoostRegressor\n",
      "AdditiveChi2Sampler\n",
      "AffinityPropagation\n",
      "AgglomerativeClustering\n",
      "BaggingClassifier\n",
      "BaggingRegressor\n",
      "BayesianGaussianMixture\n",
      "BayesianRidge\n",
      "BernoulliNB\n",
      "BernoulliRBM\n",
      "Binarizer\n",
      "Birch\n",
      "CCA\n",
      "CalibratedClassifierCV\n",
      "CheckingClassifier\n",
      "ClassifierChain\n",
      "CountVectorizer\n",
      "DBSCAN\n",
      "DPGMM\n",
      "DecisionTreeClassifier\n",
      "DecisionTreeRegressor\n",
      "DictVectorizer\n",
      "DictionaryLearning\n",
      "DummyClassifier\n",
      "DummyRegressor\n",
      "ElasticNet\n",
      "ElasticNetCV\n",
      "EllipticEnvelope\n",
      "EmpiricalCovariance\n",
      "ExtraTreeClassifier\n",
      "ExtraTreeRegressor\n",
      "ExtraTreesClassifier\n",
      "ExtraTreesRegressor\n",
      "FactorAnalysis\n",
      "FastICA\n",
      "FeatureAgglomeration\n",
      "FeatureHasher\n",
      "FeatureUnion\n",
      "FunctionTransformer\n",
      "GMM\n",
      "GaussianMixture\n",
      "GaussianNB\n",
      "GaussianProcess\n",
      "GaussianProcessClassifier\n",
      "GaussianProcessRegressor\n",
      "GaussianRandomProjection\n",
      "GaussianRandomProjectionHash\n",
      "GenericUnivariateSelect\n",
      "GradientBoostingClassifier\n",
      "GradientBoostingRegressor\n",
      "GraphLasso\n",
      "GraphLassoCV\n",
      "GridSearchCV\n",
      "GridSearchCV\n",
      "HashingVectorizer\n",
      "HuberRegressor\n",
      "Imputer\n",
      "IncrementalPCA\n",
      "IsolationForest\n",
      "Isomap\n",
      "IsotonicRegression\n",
      "KMeans\n",
      "KNeighborsClassifier\n",
      "KNeighborsRegressor\n",
      "KernelCenterer\n",
      "KernelDensity\n",
      "KernelPCA\n",
      "KernelRidge\n",
      "LSHForest\n",
      "LabelBinarizer\n",
      "LabelEncoder\n",
      "LabelPropagation\n",
      "LabelSpreading\n",
      "Lars\n",
      "LarsCV\n",
      "Lasso\n",
      "LassoCV\n",
      "LassoLars\n",
      "LassoLarsCV\n",
      "LassoLarsIC\n",
      "LatentDirichletAllocation\n",
      "LedoitWolf\n",
      "LinearDiscriminantAnalysis\n",
      "LinearRegression\n",
      "LinearSVC\n",
      "LinearSVR\n",
      "LocalOutlierFactor\n",
      "LocallyLinearEmbedding\n",
      "LogisticRegression\n",
      "LogisticRegressionCV\n",
      "MDS\n",
      "MLPClassifier\n",
      "MLPRegressor\n",
      "MaxAbsScaler\n",
      "MeanShift\n",
      "MinCovDet\n",
      "MinMaxScaler\n",
      "MiniBatchDictionaryLearning\n",
      "MiniBatchKMeans\n",
      "MiniBatchSparsePCA\n",
      "MultiLabelBinarizer\n",
      "MultiOutputClassifier\n",
      "MultiOutputRegressor\n",
      "MultiTaskElasticNet\n",
      "MultiTaskElasticNetCV\n",
      "MultiTaskLasso\n",
      "MultiTaskLassoCV\n",
      "MultinomialNB\n",
      "NMF\n",
      "NearestCentroid\n",
      "NearestNeighbors\n",
      "Normalizer\n",
      "NuSVC\n",
      "NuSVR\n",
      "Nystroem\n",
      "OAS\n",
      "OneClassSVM\n",
      "OneHotEncoder\n",
      "OneVsOneClassifier\n",
      "OneVsRestClassifier\n",
      "OrthogonalMatchingPursuit\n",
      "OrthogonalMatchingPursuitCV\n",
      "OutputCodeClassifier\n",
      "PCA\n",
      "PLSCanonical\n",
      "PLSRegression\n",
      "PLSSVD\n",
      "PassiveAggressiveClassifier\n",
      "PassiveAggressiveRegressor\n",
      "PatchExtractor\n",
      "Perceptron\n",
      "Pipeline\n",
      "PolynomialFeatures\n",
      "QuadraticDiscriminantAnalysis\n",
      "QuantileTransformer\n",
      "RANSACRegressor\n",
      "RBFSampler\n",
      "RFE\n",
      "RFECV\n",
      "RadiusNeighborsClassifier\n",
      "RadiusNeighborsRegressor\n",
      "RandomForestClassifier\n",
      "RandomForestRegressor\n",
      "RandomTreesEmbedding\n",
      "RandomizedLasso\n",
      "RandomizedLogisticRegression\n",
      "RandomizedPCA\n",
      "RandomizedSearchCV\n",
      "RandomizedSearchCV\n",
      "Ridge\n",
      "RidgeCV\n",
      "RidgeClassifier\n",
      "RidgeClassifierCV\n",
      "RobustScaler\n",
      "SGDClassifier\n",
      "SGDRegressor\n",
      "SVC\n",
      "SVR\n",
      "SelectFdr\n",
      "SelectFpr\n",
      "SelectFromModel\n",
      "SelectFwe\n",
      "SelectKBest\n",
      "SelectPercentile\n",
      "ShrunkCovariance\n",
      "SkewedChi2Sampler\n",
      "SparseCoder\n",
      "SparsePCA\n",
      "SparseRandomProjection\n",
      "SpectralBiclustering\n",
      "SpectralClustering\n",
      "SpectralCoclustering\n",
      "SpectralEmbedding\n",
      "StandardScaler\n",
      "TSNE\n",
      "TfidfTransformer\n",
      "TfidfVectorizer\n",
      "TheilSenRegressor\n",
      "TruncatedSVD\n",
      "VBGMM\n",
      "VarianceThreshold\n",
      "VotingClassifier\n",
      "_BaseRidgeCV\n",
      "_BinaryGaussianProcessClassifierLaplace\n",
      "_ConstantPredictor\n",
      "_DPGMMBase\n",
      "_GMMBase\n",
      "_RidgeGCV\n",
      "_SigmoidCalibration\n"
     ]
    }
   ],
   "source": [
    "functionListFromFile = np.array(functionListFromFile)\n",
    "for file in functionListFromFile:\n",
    "    print(file[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data from py_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AllContentQuery=  \"\"\"\\\n",
    "    SELECT repo_name, content\n",
    "    FROM  `Odyssey_github_sklearn.content_py_full`\n",
    "    WHERE (REGEXP_CONTAINS(content, \"from sklearn+ \") OR REGEXP_CONTAINS(content, \"import sklearn+ \")) AND %s \n",
    "    GROUP BY 1,2\n",
    "    \"\"\" %excludeByRepoFull(excludedRepos)\n",
    "\n",
    "AllContent = gp.run(AllContentQuery)\n",
    "\n",
    "AllContent_df = []\n",
    "\n",
    "for i in range(len(AllContent)):\n",
    "    AllContent_df.append((AllContent[i][0],AllContent[i][1]))\n",
    "    \n",
    "AllContent_df = pd.DataFrame(AllContent_df, columns= ['repo','content'])\n",
    "AllContent_df.to_csv('AllContent_df.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllContent_df=pd.read_csv('AllContent_df.csv', encoding ='utf-8')\n",
    "AllContent_py_unique_df = pd.read_csv('AllContent_unique.csv', encoding='utf-8')\n",
    "AllContent_ipynb_df =pd.read_csv('AllContent_ipynb_df.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All data from py_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AllContentQuery_unique=  \"\"\"\\\n",
    "    SELECT repo_name, content\n",
    "    FROM  `Odyssey_github_sklearn.content_py_unique`\n",
    "    WHERE (REGEXP_CONTAINS(content, \"from sklearn+ \") OR REGEXP_CONTAINS(content, \"import sklearn+ \")) AND %s \n",
    "    GROUP BY 1,2\n",
    "    \"\"\" %excludeByRepoFull(excludedRepos)\n",
    "\n",
    "AllContent_unique = gp.run(AllContentQuery_unique)\n",
    "\n",
    "AllContent_py_unique_df = []\n",
    "\n",
    "for i in range(len(AllContent_unique)):\n",
    "    AllContent_py_unique_df.append((AllContent_unique[i][0],AllContent_unique[i][1]))\n",
    "    \n",
    "AllContent_py_unique_df = pd.DataFrame(AllContent_py_unique_df, columns= ['repo','content'])\n",
    "AllContent_py_unique_df.to_csv('AllContent_unique.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_ipynb = \"\"\"\\\n",
    "SELECT\n",
    "    DISTINCT(sample_repo_name)\n",
    "FROM\n",
    "    `Odyssey_github_sklearn.content_ipynb` \n",
    "WHERE\n",
    "    (REGEXP_CONTAINS(sample_path,\"sklearn\")) OR\n",
    "    (REGEXP_CONTAINS(sample_repo_name,\"scikit-learn\")) OR\n",
    "    (REGEXP_CONTAINS(sample_repo_name,\"sklearn\")) OR\n",
    "    (REGEXP_CONTAINS(sample_path,\"scikit-learn\"))\n",
    "\"\"\"\n",
    "gp = GithubPython()\n",
    "\n",
    "\n",
    "\n",
    "excludedRepos_ipynb = [sample_repo_name[0] for sample_repo_name in gp.run(query_ipynb, project=\"odyssey-193217\")]\n",
    "print(excludedRepos_ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def excludeByRepoFull_ipynb(excludeByRepos):\n",
    "    if not excludedRepos:\n",
    "        return \"\"\n",
    "    s = '(NOT REGEXP_CONTAINS(sample_repo_name,\"%s\"))' % excludedRepos[0]\n",
    "    for repo in excludedRepos[1:]:\n",
    "        s += ' AND '\n",
    "        s += '(NOT REGEXP_CONTAINS(sample_repo_name,\"%s\"))' % repo\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AllContentQuery_ipynb=  \"\"\"\\\n",
    "    SELECT sample_repo_name, content\n",
    "    FROM  `Odyssey_github_sklearn.content_ipynb`\n",
    "    WHERE (REGEXP_CONTAINS(content, \"from sklearn+ \") OR REGEXP_CONTAINS(content, \"import sklearn+ \")) AND %s \n",
    "    GROUP BY 1,2\n",
    "    \"\"\" %excludeByRepoFull_ipynb(excludedRepos_ipynb)\n",
    "\n",
    "AllContent_ipynb = gp.run(AllContentQuery_ipynb)\n",
    "\n",
    "AllContent_ipynb_df = []\n",
    "\n",
    "for i in range(len(AllContent_ipynb)):\n",
    "    AllContent_ipynb_df.append((AllContent_ipynb[i][0],AllContent_ipynb[i][1]))\n",
    "    \n",
    "AllContent_ipynb_df = pd.DataFrame(AllContent_ipynb_df, columns=['repo','content'])\n",
    "AllContent_ipynb_df.to_csv('AllContent_ipynb_df.csv', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation analyser for py_unique\n",
    "\n",
    "#### Getting the hyperparameters for OneHotEncode in scikit-learn using Instantiation Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9246/9246 [13:09<00:00, 11.70it/s]\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_py_unique_onehotencoder = dict()\n",
    "\n",
    "for i in tqdm(range(len(AllContent_py_unique_df))):\n",
    "    \n",
    "    \n",
    "            \n",
    "    #try:\n",
    "        for file in ['OneHotEncoder']:\n",
    "            \n",
    "            if str(file) not in hyperparameter_py_unique_onehotencoder.keys():\n",
    "                hyperparameter_py_unique_onehotencoder[str(file)]={}\n",
    "                hyperparameter_py_unique_onehotencoder[str(file)][\"file_count\"] = 0\n",
    "                    \n",
    "                    \n",
    "            analyzer= InstantiationAnalyzer(str(file))\n",
    "            analyzer.parse(AllContent_py_unique_df.content[i])\n",
    "\n",
    "            if(len(analyzer.d.keys())!=0):\n",
    "                \n",
    "                hyperparameter_py_unique_onehotencoder[str(file)][\"file_count\"] += 1\n",
    "                \n",
    "                for key in analyzer.d.keys():\n",
    "                    \n",
    "                    if key not in hyperparameter_py_unique_onehotencoder[str(file)].keys():\n",
    "                        hyperparameter_py_unique_onehotencoder[str(file)][key]={}\n",
    "                        \n",
    "                    \n",
    "                    for subkey, subval in analyzer.d[key].items():\n",
    "                        \n",
    "                                             \n",
    "                        if subkey in hyperparameter_py_unique_onehotencoder[str(file)][key].keys():\n",
    "                            hyperparameter_py_unique_onehotencoder[str(file)][key][subkey] += subval\n",
    "                        else:\n",
    "                            hyperparameter_py_unique_onehotencoder[str(file)][key][subkey] = subval\n",
    "                        \n",
    "                        \n",
    "#     except AttributeError:\n",
    "#         continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OneHotEncoder': {'categorical_features': {'[0]': 1,\n",
       "   'cat': 2,\n",
       "   'category_indexes': 1,\n",
       "   'mask': 1},\n",
       "  'file_count': 7,\n",
       "  'handle_unknown': {\"'42'\": 2, \"'error'\": 2, \"'ignore'\": 2},\n",
       "  'n_values': {'2': 2,\n",
       "   '4': 2,\n",
       "   '[3, 2, 2]': 2,\n",
       "   'len(cwdict)+1': 1,\n",
       "   'len(lexdict)': 1,\n",
       "   'n_values': 1,\n",
       "   'np.int': 2},\n",
       "  'sparse': {'False': 4}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_py_unique_onehotencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the hyperparameters for all functions in scikit-learn using Instantiation Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_py_unique = dict()\n",
    "\n",
    "for i in tqdm(range(len(AllContent_py_unique_df))):\n",
    "    \n",
    "    \n",
    "            \n",
    "    try:\n",
    "        for file in functionListFromFile:\n",
    "            \n",
    "            if str(file[0]) not in hyperparameter_py_unique.keys():\n",
    "                hyperparameter_py_unique[str(file[0])]={}\n",
    "                    \n",
    "                    \n",
    "            analyzer= InstantiationAnalyzer(str(file[0]))\n",
    "            analyzer.parse(AllContent_py_unique_df.content[i])\n",
    "\n",
    "            if(len(analyzer.d.keys())!=0):\n",
    "                \n",
    "                for key in analyzer.d.keys():\n",
    "                    \n",
    "                    if key not in hyperparameter_py_unique[str(file[0])].keys():\n",
    "                        hyperparameter_py_unique[str(file[0])][key]={}\n",
    "                    \n",
    "                    for subkey, subval in analyzer.d[key].items():\n",
    "                        \n",
    "                                             \n",
    "                        if subkey in hyperparameter_py_unique[str(file[0])][key].keys():\n",
    "                            hyperparameter_py_unique[str(file[0])][key][subkey] += subval\n",
    "                        else:\n",
    "                            hyperparameter_py_unique[str(file[0])][key][subkey] = subval\n",
    "                        \n",
    "                        \n",
    "    except AttributeError:\n",
    "        continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Saving as a pickle file\n",
    "\n",
    "with open('hyperparameter_py_unique.pickle', 'wb') as handle:\n",
    "    pickle.dump(hyperparameter_py_unique, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the pickle file\n",
    "with open('hyperparameter_py_unique.pickle', 'rb') as handle:\n",
    "    hyperparameter_py_unique = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'**': {None: 4},\n",
       " 'cv': {' args.cv': 2,\n",
       "  ' self.cv': 4,\n",
       "  '(folds-1)': 2,\n",
       "  '10': 100,\n",
       "  '100': 2,\n",
       "  '2': 14,\n",
       "  '20': 14,\n",
       "  '3': 20,\n",
       "  '4': 6,\n",
       "  '5': 140,\n",
       "  'CV_K_FOLD': 8,\n",
       "  'GRID_SEARCH_CV': 2,\n",
       "  'K': 2,\n",
       "  'None': 4,\n",
       "  'ShuffleSplit(len(y), n_iter=(self.cvFolds+1), test_size=1/float(self.cvFolds), random_state=0)': 4,\n",
       "  'ShuffleSplit(len(y), n_iterations=(self.cvFolds+1), test_size=1/float(self.cvFolds), random_state=0)': 4,\n",
       "  'ShuffleSplit(len(y_train_slice), n_iter=(self.cvFolds+1), test_size=1/float(self.cvFolds), random_state=0)': 4,\n",
       "  'ShuffleSplit(n=n_samples, train_size=train_size,\\r\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tn_iter=250, random_state=1)': 2,\n",
       "  'ShuffleSplit(n=n_samples, train_size=train_size,\\r\\n                                            n_iter=250, random_state=1)': 2,\n",
       "  'ShuffleSplit(n_splits=10, test_size=0.2, random_state=10)': 2,\n",
       "  'ShuffleSplit(test_size=0.20, n_iter=1, random_state=0, n=len(train_images))': 2,\n",
       "  'StratifiedKFold(y, n_folds=3)': 2,\n",
       "  'StratifiedKFold(y_train,n_folds=cv,shuffle=True)': 4,\n",
       "  'StratifiedShuffleSplit(label, n_iter=10, test_size=0.2, train_size=None)': 2,\n",
       "  'StratifiedShuffleSplit(train_labels, n_iter=10,random_state = 42)': 2,\n",
       "  'acv': 8,\n",
       "  'cross_validation': 6,\n",
       "  'cross_validation(classes)': 2,\n",
       "  'crossfolds': 2,\n",
       "  'cv': 146,\n",
       "  'cv_method': 2,\n",
       "  'fold': 2,\n",
       "  'folds': 2,\n",
       "  'inner_cross_validation_object': 2,\n",
       "  'k_fold': 10,\n",
       "  'kf': 10,\n",
       "  'kf_cv': 2,\n",
       "  'kfold': 12,\n",
       "  'len(mags)': 2,\n",
       "  'n_crossValidation': 16,\n",
       "  'n_cv': 4,\n",
       "  'nfolds': 2,\n",
       "  'num_folds': 4,\n",
       "  'report.folds': 2,\n",
       "  'self.cv': 4,\n",
       "  'skf': 2,\n",
       "  'split': 2,\n",
       "  'ss': 2,\n",
       "  'sss': 2,\n",
       "  'strat_2fold': 2},\n",
       " 'error_score': {'0': 4},\n",
       " 'estimator': {'DecisionTreeClassifier(random_state=0)': 2,\n",
       "  \"LogisticRegression(C=1.0,penalty='l1',random_state=0, tol=1e-4)\": 2,\n",
       "  \"LogisticRegression(C=1.0,penalty='l2',random_state=0, tol=1e-4)\": 2,\n",
       "  'Perceptron()': 2,\n",
       "  'SVM_pipeline': 2,\n",
       "  \"XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=4,\\\\\\r\\nmin_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\\\\\\r\\nobjective='binary:logistic', nthread=4, scale_pos_weight=1,seed=27)\": 2,\n",
       "  \"XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,\\\\\\r\\nmin_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\\\\\\r\\nobjective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\": 2,\n",
       "  \"XGBClassifier(learning_rate=0.1, n_estimators=140, max_depth=5,\\\\\\r\\nmin_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\\\\\\r\\nobjective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\": 2,\n",
       "  'alg': 2,\n",
       "  'classifier': 2,\n",
       "  'classifier_tune': 4,\n",
       "  'clf': 10,\n",
       "  'clf_dict[model_name][\"clf\"]': 2,\n",
       "  'eclf': 8,\n",
       "  'estimator': 20,\n",
       "  'gb': 2,\n",
       "  'gbm': 2,\n",
       "  'gmm': 2,\n",
       "  'model': 20,\n",
       "  'mv_clf': 8,\n",
       "  'naive_bayes.MultinomialNB()': 2,\n",
       "  'neighbors.KNeighborsClassifier()': 2,\n",
       "  'neural_net': 2,\n",
       "  'pipe_svc': 10,\n",
       "  'pipeline': 4,\n",
       "  'pips[i]': 4,\n",
       "  'rf': 8,\n",
       "  'sclf': 8,\n",
       "  'select': 2,\n",
       "  'self.__clfs[clf]': 2,\n",
       "  'self.clf': 10,\n",
       "  'self.gbc_base': 2,\n",
       "  'svc': 4,\n",
       "  'svm.SVC()': 6,\n",
       "  'svm.SVR()': 2,\n",
       "  'svm_clsf': 2,\n",
       "  'svr': 2,\n",
       "  'tuned_clf': 8},\n",
       " 'fit_params': {'fit_params': 4,\n",
       "  \"{'sample_weight': balance_weights(y_train)}\": 2},\n",
       " 'iid': {'False': 16, 'True': 2, 'iid': 2},\n",
       " 'n_jobs': {' -1': 14,\n",
       "  ' cpu_count()': 4,\n",
       "  ' self.cvJobs': 20,\n",
       "  '-1': 122,\n",
       "  '1': 18,\n",
       "  '10': 6,\n",
       "  '2': 24,\n",
       "  '20': 10,\n",
       "  '24': 6,\n",
       "  '3': 24,\n",
       "  '4': 34,\n",
       "  '5': 2,\n",
       "  '6': 10,\n",
       "  '8': 4,\n",
       "  'NB_THREADS': 2,\n",
       "  'N_JOBS': 12,\n",
       "  'PARAM_JOBS': 10,\n",
       "  'SVM_PARAM_JOBS': 2,\n",
       "  'cpu': 2,\n",
       "  'cpus': 2,\n",
       "  'jobs': 2,\n",
       "  'multiprocessing.cpu_count()': 2,\n",
       "  'n_cpus': 2,\n",
       "  'n_jobs': 48,\n",
       "  'n_processes': 16,\n",
       "  'njobs()': 2,\n",
       "  'self.jobs': 4,\n",
       "  'self.n_jobs': 2,\n",
       "  'threads': 2},\n",
       " 'param_grid': {'SVM_parameters': 2,\n",
       "  \"[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}]\": 2,\n",
       "  'clf_dict[model_name][\"paramteters\"]': 2,\n",
       "  'dict(C=CS)': 2,\n",
       "  'dict(C=C_values)': 2,\n",
       "  'dict(C=Cs, gamma=Gammas)': 2,\n",
       "  'dict(alpha=alphas)': 2,\n",
       "  'dict(alpha=numpy.linspace(0,2,20)[1:])': 2,\n",
       "  'dict(alpha=numpy.linspace(0,2,20)[1:],n_iter=[iterations])': 2,\n",
       "  'dict(gamma=gammas)': 2,\n",
       "  'dict(gamma=gammas, C=C_values)': 2,\n",
       "  'dict(n_estimators=es, learning_rate=ls)': 2,\n",
       "  'dict(n_estimators=es, max_features=fs)': 2,\n",
       "  'dict(n_estimators=n_est, max_depth=m_dep)': 2,\n",
       "  'dict(n_estimators=n_estimators, max_depth=max_depth)': 2,\n",
       "  'gbc_search_params': 2,\n",
       "  'grid': 4,\n",
       "  'grid_parameters': 2,\n",
       "  'grid_params': 2,\n",
       "  'linear_params': 2,\n",
       "  'obj': 2,\n",
       "  'paramGrid': 20,\n",
       "  'param_grid': 206,\n",
       "  'param_grid[name]': 2,\n",
       "  'param_grid_gbm': 2,\n",
       "  'param_grid_knn': 4,\n",
       "  'param_grid_rbf': 2,\n",
       "  'param_grid_rf': 6,\n",
       "  'param_grid_svm': 4,\n",
       "  'param_pol_grid': 2,\n",
       "  'param_rbf_grid': 2,\n",
       "  'param_test1': 2,\n",
       "  'param_test2': 2,\n",
       "  'param_test3': 2,\n",
       "  'parameter_candidates': 2,\n",
       "  'parameter_grid': 6,\n",
       "  'parameters': 24,\n",
       "  'params': 40,\n",
       "  'params2Test': 2,\n",
       "  'pars[i]': 4,\n",
       "  'passed_parameters': 2,\n",
       "  'poly_params': 2,\n",
       "  'rbf_params': 2,\n",
       "  'self.params': 2,\n",
       "  'tuned_parameters': 2,\n",
       "  'union_parameters': 2,\n",
       "  \"{\\r\\n    'C' :[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,0.9],\\r\\n    'kernel':['rbf','sigmoid']\\r\\n       }\": 2,\n",
       "  \"{\\r\\n    'solver': ['newton-cg','liblinear','lbfgs', 'sag'],\\r\\n    'tol':[0.00001, 0.0001],\\r\\n    'C':[0.2, 0.5, 0.80]  #[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\\r\\n       }\": 2,\n",
       "  \"{ 'C': [1, 2, 3, 4, 5],\\r\\n                'kernel': [ 'linear', 'poly', 'rbf', 'sigmoid' ] }\": 2,\n",
       "  '{\"C\": 2**np.arange(1,14),\\r\\n                                   \"gamma\": np.logspace(-2, 2, 10),\\r\\n                                    \"epsilon\" : [0, 0.01, 0.1, 0.5]}': 2,\n",
       "  '{\"C\": np.logspace(-2, 2, 20),\"gamma\": np.logspace(-2, 2, 20)}': 2,\n",
       "  \"{'C': list(range(1,1000,10)), 'gamma': np.arange(0.0001, 0.001,0.001), 'kernel': ['rbf'], 'class_weight':[{'1':weightNormal,'-1':weightFake}]}\": 2,\n",
       "  \"{'estimator_params': param_grid}\": 2,\n",
       "  \"{'probability': [True]}\": 2},\n",
       " 'pre_dispatch': {\"'1.5*n_jobs'\": 4, \"'2*n_jobs'\": 6, '2': 4, 'n_jobs': 8},\n",
       " 'refit': {'False': 8, 'True': 14, 'refit': 2},\n",
       " 'return_train_score': {'False': 4},\n",
       " 'score_func': {\"'mean_absolute_error'\": 2,\n",
       "  \"'roc_auc'\": 2,\n",
       "  'accus': 4,\n",
       "  'precision_score': 12,\n",
       "  'score': 2,\n",
       "  'score_func': 2},\n",
       " 'scoring': {' self.scoring': 4,\n",
       "  '\"accuracy\"': 6,\n",
       "  '\"f1\"': 20,\n",
       "  '\"precision\"': 6,\n",
       "  '\"recall\"': 4,\n",
       "  '\"roc_auc\"': 18,\n",
       "  \"'%s_macro' % score\": 8,\n",
       "  \"'%s_weighted' % score\": 4,\n",
       "  \"'accuracy'\": 66,\n",
       "  \"'f1'\": 54,\n",
       "  \"'mean_squared_error'\": 12,\n",
       "  \"'neg_mean_squared_error'\": 12,\n",
       "  \"'precision'\": 6,\n",
       "  \"'precision_macro'\": 2,\n",
       "  \"'r2'\": 2,\n",
       "  \"'recall'\": 16,\n",
       "  \"'roc_auc'\": 48,\n",
       "  'SCORING': 12,\n",
       "  'Scorer': 2,\n",
       "  \"cfg['scoring']\": 2,\n",
       "  'evaluation_measure': 2,\n",
       "  'f1_scorer': 2,\n",
       "  \"format('{}_macro'.format(score))\": 2,\n",
       "  'ftwo_scorer': 12,\n",
       "  'grid_score': 2,\n",
       "  'make_scorer(error_function, greater_is_better=False)': 2,\n",
       "  'make_scorer(jaccard_similarity_score)': 2,\n",
       "  'measure': 2,\n",
       "  'metric': 2,\n",
       "  'model_selection_measure': 2,\n",
       "  'p_value_scoring_object.p_value_scoring_object': 6,\n",
       "  'p_value_scoring_object.p_value_scoring_object_AD': 2,\n",
       "  'power_at_5pc_FPR': 2,\n",
       "  'score': 24,\n",
       "  'scoreParam': 4,\n",
       "  'score_fn': 2,\n",
       "  'scorer': 14,\n",
       "  'scoring': 8,\n",
       "  \"self.config['ml']['metric']\": 2,\n",
       "  'self.scoring': 2,\n",
       "  'self.scoring_metric': 2,\n",
       "  'spearman_scoring': 24},\n",
       " 'verbose': {' self.verbose': 4,\n",
       "  '0': 10,\n",
       "  '1': 78,\n",
       "  '10': 18,\n",
       "  '100': 2,\n",
       "  '2': 12,\n",
       "  '3': 2,\n",
       "  '5': 8,\n",
       "  '50': 2,\n",
       "  '999': 2,\n",
       "  'PARAM_VERBOSE': 14,\n",
       "  'True': 4,\n",
       "  'verbose': 8,\n",
       "  'verboseLevel': 16}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_py_unique['GridSearchCV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting ipython notebook files with Pipeline usage (with InstantiationAnalyser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ipynb_pipeline_files = []\n",
    "\n",
    "for index in tqdm(range(len(AllContent_ipynb_df))):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        py_file = \"ipython_files/file\" +str(index)+\".py\"\n",
    "\n",
    "        with open(py_file, 'r',encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "\n",
    "        analyzer= InstantiationAnalyzer(\"Pipeline\")\n",
    "        analyzer.parse(file_content)\n",
    "\n",
    "        if(len(analyzer.d.keys())!=0):\n",
    "\n",
    "            ipynb_pipeline_files.append(index)\n",
    "            \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "with open('ipynb_pipeline_files.pickle', 'wb') as handle:\n",
    "    pickle.dump(ipynb_pipeline_files, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation analyser for ipynb files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping all json ipynb to py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(AllContent_ipynb_df)):\n",
    "    \n",
    "    file_name = \"ipython_files/file\"+str(i)+\".ipynb \"\n",
    "    print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list of invalid ipython files\n",
    "\n",
    "excepted_files = []\n",
    "for i in tqdm(range(len(AllContent_ipynb_df))):\n",
    "    \n",
    "    imp_file_name= \"ipython_files/file\"+str(i)+\".ipynb\"\n",
    "    exp_file_name= \"ipython_files/file\"+str(i)+\".py\"\n",
    "    \n",
    "    python_exp = PythonExporter()\n",
    "    \n",
    "    try:\n",
    "  \n",
    "        data = python_exp.from_file(imp_file_name)\n",
    "\n",
    "        with open(exp_file_name, 'w',encoding='utf-8') as file:\n",
    "            file.write(str(data))\n",
    "            \n",
    "            \n",
    "    except:\n",
    "        excepted_files.append((imp_file_name+\"\\n\"))\n",
    "        continue\n",
    "    \n",
    "              \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the hyperparameters for OneHotEncoder in scikit-learn using Instantiation Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/4372 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 4/4372 [00:00<02:26, 29.76it/s]\n",
      "  0%|                                                                                 | 5/4372 [00:00<08:09,  8.93it/s]\n",
      "  0%|▏                                                                                | 7/4372 [00:00<07:06, 10.23it/s]\n",
      "  0%|▏                                                                                | 9/4372 [00:00<07:15, 10.01it/s]\n",
      "  0%|▏                                                                               | 10/4372 [00:01<07:47,  9.32it/s]\n",
      "  0%|▏                                                                               | 11/4372 [00:01<07:46,  9.35it/s]\n",
      "  0%|▏                                                                               | 12/4372 [00:01<08:39,  8.40it/s]\n",
      "  0%|▏                                                                               | 13/4372 [00:01<09:12,  7.89it/s]\n",
      "  0%|▎                                                                               | 14/4372 [00:03<17:12,  4.22it/s]\n",
      "  0%|▎                                                                               | 15/4372 [00:03<17:01,  4.27it/s]\n",
      "  0%|▎                                                                               | 16/4372 [00:03<16:54,  4.29it/s]\n",
      "  0%|▎                                                                               | 17/4372 [00:03<16:49,  4.32it/s]\n",
      "  0%|▎                                                                               | 20/4372 [00:04<15:16,  4.75it/s]\n",
      "  0%|▍                                                                               | 21/4372 [00:04<15:06,  4.80it/s]\n",
      "  1%|▍                                                                               | 23/4372 [00:04<14:19,  5.06it/s]\n",
      "  1%|▍                                                                               | 25/4372 [00:04<14:04,  5.15it/s]\n",
      "  1%|▍                                                                               | 27/4372 [00:05<13:28,  5.37it/s]\n",
      "  1%|▌                                                                               | 28/4372 [00:05<13:44,  5.27it/s]\n",
      "  1%|▌                                                                               | 29/4372 [00:05<14:18,  5.06it/s]\n",
      "  1%|▌                                                                               | 31/4372 [00:05<13:40,  5.29it/s]\n",
      "  1%|▌                                                                               | 33/4372 [00:06<13:38,  5.30it/s]\n",
      "  1%|▋                                                                               | 35/4372 [00:06<13:20,  5.42it/s]\n",
      "  1%|▋                                                                               | 36/4372 [00:06<13:12,  5.47it/s]\n",
      "  1%|▋                                                                               | 37/4372 [00:06<13:24,  5.39it/s]\n",
      "  1%|▋                                                                               | 38/4372 [00:07<13:24,  5.39it/s]\n",
      "  1%|▋                                                                               | 39/4372 [00:07<13:28,  5.36it/s]\n",
      "  1%|▋                                                                               | 40/4372 [00:07<13:30,  5.35it/s]\n",
      "  1%|▊                                                                               | 42/4372 [00:07<13:14,  5.45it/s]\n",
      "  1%|▊                                                                               | 43/4372 [00:08<14:08,  5.10it/s]\n",
      "  1%|▊                                                                               | 45/4372 [00:08<13:49,  5.22it/s]\n",
      "  1%|▊                                                                               | 47/4372 [00:08<13:26,  5.36it/s]\n",
      "  1%|▉                                                                               | 49/4372 [00:09<13:14,  5.44it/s]\n",
      "  1%|▉                                                                               | 51/4372 [00:09<12:54,  5.58it/s]\n",
      "  1%|▉                                                                               | 53/4372 [00:09<12:38,  5.69it/s]\n",
      "  1%|█                                                                               | 55/4372 [00:09<12:25,  5.79it/s]\n",
      "  1%|█                                                                               | 57/4372 [00:09<12:14,  5.87it/s]\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aishg\\Anaconda3\\lib\\threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\aishg\\Anaconda3\\lib\\site-packages\\tqdm\\_monitor.py\", line 63, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\aishg\\Anaconda3\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 4372/4372 [06:12<00:00, 11.73it/s]\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_ipynb_onehotencoder = dict()\n",
    "\n",
    "for py_file in tqdm(glob.glob('ipython_files/*.py')):\n",
    "    \n",
    "    with open(py_file, 'r',encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "                \n",
    "               \n",
    "   # try:\n",
    "        for function_name in ['OneHotEncoder']:\n",
    "            \n",
    "            if str(function_name) not in hyperparameter_ipynb_onehotencoder.keys():\n",
    "                hyperparameter_ipynb_onehotencoder[str(function_name)]={}\n",
    "                hyperparameter_ipynb_onehotencoder[str(function_name)][\"file_count\"]=0\n",
    "                    \n",
    "                    \n",
    "            analyzer= InstantiationAnalyzer(str(function_name))             \n",
    "            analyzer.parse(file_content)\n",
    "            \n",
    "            if(len(analyzer.d.keys())!=0):\n",
    "                \n",
    "                hyperparameter_ipynb_onehotencoder[str(function_name)][\"file_count\"]+=1\n",
    "                \n",
    "                for key in analyzer.d.keys():\n",
    "                    \n",
    "                    if key not in hyperparameter_ipynb_onehotencoder[str(function_name)].keys():\n",
    "                        hyperparameter_ipynb_onehotencoder[str(function_name)][key]={}\n",
    "                    \n",
    "                    for subkey, subval in analyzer.d[key].items():\n",
    "                        \n",
    "                                             \n",
    "                        if subkey in hyperparameter_ipynb_onehotencoder[str(function_name)][key].keys():\n",
    "                            hyperparameter_ipynb_onehotencoder[str(function_name)][key][subkey] += subval\n",
    "                        else:\n",
    "                            hyperparameter_ipynb_onehotencoder[str(function_name)][key][subkey] = subval\n",
    "                        \n",
    "                        \n",
    "#     except AttributeError:\n",
    "#         continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OneHotEncoder': {'categorical_features': {\"'all'\": 1,\n",
       "   '[0, 1, 4, 7, 15, 26, 27]': 6,\n",
       "   '[0]': 2,\n",
       "   'categ': 2,\n",
       "   'categorical_indexes': 1,\n",
       "   'pd.Series(select_features).isin(categorical_features)': 2},\n",
       "  'dtype': {'int': 2},\n",
       "  'file_count': 9,\n",
       "  'n_values': {\"mdl['encoder']['values']\": 1},\n",
       "  'sparse': {'False': 2, 'True': 2}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_ipynb_onehotencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving in pickle file\n",
    "\n",
    "with open('hyperparameter_ipynb_onehotencoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(hyperparameter_ipynb_onehotencoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the hyperparameters for all functions in scikit-learn using Instantiation Analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ipynb = dict()\n",
    "\n",
    "for py_file in tqdm(glob.glob('ipython_files/*.py')):\n",
    "    \n",
    "    with open(py_file, 'r',encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "                \n",
    "               \n",
    "    try:\n",
    "        for function_name in functionListFromFile:\n",
    "            \n",
    "            if str(function_name[0]) not in hyperparameter_ipynb.keys():\n",
    "                hyperparameter_ipynb[str(function_name[0])]={}\n",
    "                    \n",
    "                    \n",
    "            analyzer= InstantiationAnalyzer(str(function_name[0]))             \n",
    "            analyzer.parse(file_content)\n",
    "            \n",
    "            if(len(analyzer.d.keys())!=0):\n",
    "                \n",
    "               # print(\"key not zero\")\n",
    "                \n",
    "                for key in analyzer.d.keys():\n",
    "                    \n",
    "                    if key not in hyperparameter_ipynb[str(function_name[0])].keys():\n",
    "                        hyperparameter_ipynb[str(function_name[0])][key]={}\n",
    "                    \n",
    "                    for subkey, subval in analyzer.d[key].items():\n",
    "                        \n",
    "                                             \n",
    "                        if subkey in hyperparameter_ipynb[str(function_name[0])][key].keys():\n",
    "                            hyperparameter_ipynb[str(function_name[0])][key][subkey] += subval\n",
    "                        else:\n",
    "                            hyperparameter_ipynb[str(function_name[0])][key][subkey] = subval\n",
    "                        \n",
    "                        \n",
    "    except AttributeError:\n",
    "        continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving in pickle file\n",
    "\n",
    "with open('hyperparameter_ipynb.pickle', 'wb') as handle:\n",
    "    pickle.dump(hyperparameter_ipynb, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading the pickle file\n",
    "with open('hyperparameter_ipynb.pickle', 'rb') as handle:\n",
    "    hyperparameter_ipynb = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' int(depth)': 1,\n",
       " '10': 18,\n",
       " '100': 6,\n",
       " '12': 4,\n",
       " '14': 1,\n",
       " '15': 33,\n",
       " '150': 1,\n",
       " '16': 3,\n",
       " '2': 2,\n",
       " '22': 2,\n",
       " '25': 3,\n",
       " '3': 6,\n",
       " '30': 2,\n",
       " '4': 4,\n",
       " '5': 55,\n",
       " '50': 1,\n",
       " '6': 10,\n",
       " '7': 16,\n",
       " '8': 14,\n",
       " '9': 1,\n",
       " 'None': 36,\n",
       " 'best_m': 1,\n",
       " 'best_max_depth': 3,\n",
       " 'depth': 1,\n",
       " 'g': 2,\n",
       " 'i': 2,\n",
       " 'm': 1,\n",
       " 'max_depth': 3}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameter_ipynb['RandomForestClassifier']['max_depth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the scikit-learn module usage with jedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 2954/2954 [12:24:17<00:00, 15.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# # Module Usage analysis \n",
    "\n",
    "module_analysis_py_unique = pd.DataFrame(columns=['Filename_index','Description','Package/Module', 'Usage'])\n",
    "for i in tqdm(range(0, len(AllContent_py_unique_df))):   \n",
    "\n",
    "    source = AllContent_py_unique_df.content[i]\n",
    "\n",
    "   \n",
    "    for pos in range(len(source.splitlines())):\n",
    "        \n",
    "        try:\n",
    "                \n",
    "            script = jedi.Script(source,pos)\n",
    "            d = script.goto_definitions()[0]\n",
    "            if \"sklearn\" in str(d.full_name):\n",
    "                \n",
    "                temp_df = pd.DataFrame([[i,d.description,d.full_name, source.splitlines()[pos-1]]], columns=['Filename_index','Description','Package/Module', 'Usage'])\n",
    "                module_analysis_py_unique = module_analysis_py_unique.append(temp_df, ignore_index=True)\n",
    "            \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "            \n",
    "cols = list(module_analysis_py_unique.columns)\n",
    "module_analysis_py_unique['count'] = module_analysis_py_unique.groupby(cols)['Filename_index'].transform('size')\n",
    "module_analysis_py_unique.drop_duplicates(inplace=True)\n",
    "module_analysis_py_unique.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('module_analysis_py_unique.pickle', 'wb') as handle:\n",
    "    pickle.dump(module_analysis_py_unique, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('module_analysis_py_unique.pickle', 'rb') as handle:\n",
    "    module_analysis_py_unique = pickle.load(handle)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter analysis of pipeline before and after jedi parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_file_py_unique = module_analysis_py_unique['Filename_index'].unique()\n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 8860/8860 [10:20<00:00, 14.27it/s]\n"
     ]
    }
   ],
   "source": [
    "## PIPELINE ANALYSIS AFTER JEDI\n",
    "\n",
    "\n",
    "hyperparameter_py_unique_after_jedi_Pipeline = dict()\n",
    "after_jedi_pipeline_files=[]\n",
    "\n",
    "for i in tqdm(MA_file_py_unique):\n",
    "            \n",
    "    try:       \n",
    "            \n",
    "        analyzer= InstantiationAnalyzer(\"Pipeline\")\n",
    "        analyzer.parse(AllContent_py_unique_df.content[i])\n",
    "\n",
    "        if(len(analyzer.d.keys())!=0):\n",
    "            \n",
    "            after_jedi_pipeline_files.append(i)\n",
    "\n",
    "            for key in analyzer.d.keys():\n",
    "                \n",
    "                if key not in hyperparameter_py_unique_after_jedi_Pipeline.keys():\n",
    "                    hyperparameter_py_unique_after_jedi_Pipeline[key]={}\n",
    "\n",
    "                for subkey, subval in analyzer.d[key].items():\n",
    "\n",
    "\n",
    "                    if subkey in hyperparameter_py_unique_after_jedi_Pipeline[key].keys():\n",
    "                        hyperparameter_py_unique_after_jedi_Pipeline[key][subkey] += subval\n",
    "                    else:\n",
    "                        hyperparameter_py_unique_after_jedi_Pipeline[key][subkey] = subval\n",
    "                        \n",
    "                        \n",
    "    except AttributeError:\n",
    "        continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving to pickle file\n",
    "with open('hyperparameter_py_unique_after_jedi_Pipeline.pickle', 'wb') as handle:\n",
    "    pickle.dump(hyperparameter_py_unique_after_jedi_Pipeline, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('after_jedi_pipeline_files.pickle', 'wb') as handle:\n",
    "    pickle.dump(after_jedi_pipeline_files, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PIPELINE ANALYSIS AFTER JEDI\n",
    "\n",
    "\n",
    "hyperparameter_py_unique_before_jedi_Pipeline = dict()\n",
    "before_jedi_pipeline_files=[]\n",
    "\n",
    "for i in tqdm(range(len(AllContent_py_unique_df))):\n",
    "            \n",
    "    try:\n",
    "            \n",
    "        analyzer= InstantiationAnalyzer(\"Pipeline\")\n",
    "        analyzer.parse(AllContent_py_unique_df.content[i])\n",
    "\n",
    "        if(len(analyzer.d.keys())!=0):\n",
    "            \n",
    "            before_jedi_pipeline_files.append(i)\n",
    "            \n",
    "\n",
    "            for key in analyzer.d.keys():\n",
    "\n",
    "                if key not in hyperparameter_py_unique_before_jedi_Pipeline.keys():\n",
    "                    hyperparameter_py_unique_before_jedi_Pipeline[key]={}\n",
    "\n",
    "                for subkey, subval in analyzer.d[key].items():\n",
    "\n",
    "\n",
    "                    if subkey in hyperparameter_py_unique_before_jedi_Pipeline[key].keys():\n",
    "                        hyperparameter_py_unique_before_jedi_Pipeline[key][subkey] += subval\n",
    "                    else:\n",
    "                        hyperparameter_py_unique_before_jedi_Pipeline[key][subkey] = subval\n",
    "                        \n",
    "                        \n",
    "    except AttributeError:\n",
    "        continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving to pickle filea\n",
    "with open('hyperparameter_py_unique_before_jedi_Pipeline.pickle', 'wb') as handle:\n",
    "    pickle.dump(hyperparameter_py_unique_before_jedi_Pipeline, handle, protocol=pickle.HIGHEST_PROTOCOL)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('after_jedi_pipeline_files.pickle', 'rb') as handle:\n",
    "    after_jedi_pipeline_files = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract files containing make_pipiline using BiqQuery API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_pipelineQuery = \"\"\"\\\n",
    "    SELECT repo_name, line\n",
    "    FROM (\n",
    "          SELECT DISTINCT(line), repo_name\n",
    "          FROM(\n",
    "          SELECT (SPLIT(content, '\\\\n')) as lines, repo_name, content\n",
    "          FROM\n",
    "          `Odyssey_github_sklearn.content_py_unique`\n",
    "          ), UNNEST(lines) line \n",
    "\n",
    "          WHERE (REGEXP_CONTAINS(line, 'make_pipeline')) AND NOT (REGEXP_CONTAINS(line, 'print')) AND NOT (REGEXP_CONTAINS(line, 'import')) AND (REGEXP_CONTAINS(content, 'from sklearn+  import') OR REGEXP_CONTAINS(content, 'import sklearn+ ')) AND %s\n",
    "         )\n",
    "     \n",
    "        GROUP BY 1,2\n",
    "    \n",
    "        \"\"\" %( excludeByRepoFull(excludedRepos))\n",
    "\n",
    "    \n",
    "    \n",
    "make_pipeline_result = gp.run(make_pipelineQuery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
